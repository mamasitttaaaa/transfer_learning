{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение трансферного обучения с традиционными моделями в задаче выявления сетевых атак\n",
    "\n",
    "Трансферное обучение — это метод машинного обучения, при котором модель, обученная на одной задаче, повторно используется для другой, схожей задачи. Вместо того, чтобы обучать модель с нуля, используется уже существующая модель, адаптируя её к новой задаче. Это помогает сократить время обучения и улучшить результаты, особенно если у новой задачи недостаточно данных.\n",
    "\n",
    "В работе для трансферного обучения используются данные из открытых наборов о кибератаках UNSW-NB15 и Bot-IoT. В работе будет использоваться только 5% данных, набор рекомендованный самими составителями.\n",
    "\n",
    "Набор UNSW-NB15 содержит сетевые данные, собранные с помощью генератора пакетов, генерирующего нормальный трафик и различные типы атак.\n",
    "\n",
    "Набор Bot-IoT содержит большое количество реальных данных, собранных в области Интернета вещей с симуляцией нормального и атакующего трафика.\n",
    "\n",
    "Таким образом, эти наборы имеют различия и при использовании модели, обученной на одном, будет низкая эффективность обнаружения угроз при валидации данных из второго набора. Поэтому из-за схожести задачи  данные можно исользовать при трансферном обучении, которое в данном случае может быть успешным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исходные данные обоих файлов сравниваются по признакам, выявляются общие и только на основе них составляются наборы для обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7m/yq3bdqw12mj3g98dqwm4yk180000gn/T/ipykernel_25606/175988168.py:14: DtypeWarning: Columns (1,3,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df1 = pd.read_csv('UNSW-NB15_1.csv', header=None, names=headers)\n",
      "/var/folders/7m/yq3bdqw12mj3g98dqwm4yk180000gn/T/ipykernel_25606/175988168.py:15: DtypeWarning: Columns (3,39,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df2 = pd.read_csv('UNSW-NB15_2.csv', header=None, names=headers)\n",
      "/var/folders/7m/yq3bdqw12mj3g98dqwm4yk180000gn/T/ipykernel_25606/175988168.py:41: DtypeWarning: Columns (7,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/var/folders/7m/yq3bdqw12mj3g98dqwm4yk180000gn/T/ipykernel_25606/175988168.py:41: DtypeWarning: Columns (7,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/var/folders/7m/yq3bdqw12mj3g98dqwm4yk180000gn/T/ipykernel_25606/175988168.py:41: DtypeWarning: Columns (7,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n",
      "/var/folders/7m/yq3bdqw12mj3g98dqwm4yk180000gn/T/ipykernel_25606/175988168.py:41: DtypeWarning: Columns (7,9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер тренировочного набора UNSW-NB15: (869762, 153)\n",
      "Размер тестового набора UNSW-NB15: (217441, 153)\n",
      "Размер тренировочного набора BoT-IoT: (2934817, 153)\n",
      "Размер тестового набора BoT-IoT: (733705, 153)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# загрузка файла с признаками\n",
    "df_features = pd.read_csv('NUSW-NB15_features.csv', encoding='latin1')\n",
    "\n",
    "# Извлечение заголовков\n",
    "headers = df_features[\"Name\"].tolist()\n",
    "\n",
    "# загрузка основных файлов с данными\n",
    "df1 = pd.read_csv('UNSW-NB15_1.csv', header=None, names=headers)\n",
    "df2 = pd.read_csv('UNSW-NB15_2.csv', header=None, names=headers)\n",
    "df3 = pd.read_csv('UNSW-NB15_3.csv', header=None, names=headers)\n",
    "df4 = pd.read_csv('UNSW-NB15_4.csv', header=None, names=headers)\n",
    "\n",
    "df_main = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "\n",
    "# --- Работа с первым набором данных (UNSW-NB15) ---\n",
    "columns_to_drop = ['srcip', 'dstip', 'sport', 'dsport', 'Stime', 'Ltime']\n",
    "df_main = df_main.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Удаление строк с пропусками (кроме attack_cat)\n",
    "df_main_cleaned = df_main.dropna(subset=[col for col in df_main.columns if col != 'attack_cat'])\n",
    "\n",
    "# Разделение данных на признаки и целевую переменную\n",
    "X_unsw = df_main_cleaned.drop(columns=['Label', 'attack_cat'], errors='ignore')\n",
    "y_unsw = df_main_cleaned['Label']\n",
    "\n",
    "# --- Работа со вторым набором данных (BoT-IoT) ---\n",
    "file_paths = [\n",
    "    'UNSW_2018_IoT_Botnet_Full5pc_1.csv',\n",
    "    'UNSW_2018_IoT_Botnet_Full5pc_2.csv',\n",
    "    'UNSW_2018_IoT_Botnet_Full5pc_3.csv',\n",
    "    'UNSW_2018_IoT_Botnet_Full5pc_4.csv'\n",
    "]\n",
    "dataframes = []\n",
    "for file in file_paths:\n",
    "    df = pd.read_csv(file)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Объединение файлов\n",
    "df_bot = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Приведение признаков BoT-IoT к признакам UNSW-NB15\n",
    "unsw_nb15_features = X_unsw.columns.tolist()  # Признаки первого набора\n",
    "common_features = list(set(df_bot.columns).intersection(unsw_nb15_features))\n",
    "common_features.append('attack')  # Добавление целевой переменной\n",
    "df_bot = df_bot[common_features]\n",
    "df_bot.rename(columns={'attack': 'Label'}, inplace=True)\n",
    "\n",
    "# Разделение данных на признаки и целевую переменную\n",
    "X_bot = df_bot.drop(columns=['Label'], errors='ignore')\n",
    "y_bot = df_bot['Label']\n",
    "\n",
    "# --- Выравнивание признаков ---\n",
    "common_features = list(set(X_unsw.columns).intersection(X_bot.columns))\n",
    "X_unsw = X_unsw[common_features]\n",
    "X_bot = X_bot[common_features]\n",
    "\n",
    "# --- Предобработка данных ---\n",
    "categorical_features = X_unsw.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = X_unsw.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', MinMaxScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# Обработка UNSW-NB15\n",
    "X_unsw_train, X_unsw_test, y_unsw_train, y_unsw_test = train_test_split(X_unsw, y_unsw, test_size=0.2, random_state=42)\n",
    "X_unsw_train_processed = pipeline.fit_transform(X_unsw_train)\n",
    "X_unsw_test_processed = pipeline.transform(X_unsw_test)\n",
    "\n",
    "# Обработка BoT-IoT\n",
    "X_bot_train, X_bot_test, y_bot_train, y_bot_test = train_test_split(X_bot, y_bot, test_size=0.2, random_state=42)\n",
    "X_bot_train_processed = pipeline.transform(X_bot_train)\n",
    "X_bot_test_processed = pipeline.transform(X_bot_test)\n",
    "\n",
    "# Проверка размеров\n",
    "print(f\"Размер тренировочного набора UNSW-NB15: {X_unsw_train_processed.shape}\")\n",
    "print(f\"Размер тестового набора UNSW-NB15: {X_unsw_test_processed.shape}\")\n",
    "print(f\"Размер тренировочного набора BoT-IoT: {X_bot_train_processed.shape}\")\n",
    "print(f\"Размер тестового набора BoT-IoT: {X_bot_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение модели на данных UNSW-NB15:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m21745/21745\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1ms/step - accuracy: 0.9819 - loss: 0.0673 - val_accuracy: 0.9842 - val_loss: 0.0587\n",
      "Epoch 2/10\n",
      "\u001b[1m21745/21745\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 1ms/step - accuracy: 0.9833 - loss: 0.0612 - val_accuracy: 0.9842 - val_loss: 0.0578\n",
      "Epoch 3/10\n",
      "\u001b[1m21745/21745\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 1ms/step - accuracy: 0.9832 - loss: 0.0590 - val_accuracy: 0.9823 - val_loss: 0.0559\n",
      "Epoch 4/10\n",
      "\u001b[1m21745/21745\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1ms/step - accuracy: 0.9830 - loss: 0.0584 - val_accuracy: 0.9842 - val_loss: 0.0533\n",
      "Epoch 5/10\n",
      "\u001b[1m21745/21745\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1ms/step - accuracy: 0.9834 - loss: 0.0565 - val_accuracy: 0.9842 - val_loss: 0.0541\n",
      "Epoch 6/10\n",
      "\u001b[1m21745/21745\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2ms/step - accuracy: 0.9836 - loss: 0.0544 - val_accuracy: 0.9825 - val_loss: 0.0555\n",
      "Epoch 7/10\n",
      "\u001b[1m21745/21745\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1ms/step - accuracy: 0.9834 - loss: 0.0542 - val_accuracy: 0.9842 - val_loss: 0.0499\n",
      "Epoch 8/10\n",
      "\u001b[1m21745/21745\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1ms/step - accuracy: 0.9835 - loss: 0.0532 - val_accuracy: 0.9842 - val_loss: 0.0533\n",
      "Epoch 9/10\n",
      "\u001b[1m21745/21745\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1ms/step - accuracy: 0.9834 - loss: 0.0533 - val_accuracy: 0.9842 - val_loss: 0.0534\n",
      "Epoch 10/10\n",
      "\u001b[1m21745/21745\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 1ms/step - accuracy: 0.9834 - loss: 0.0526 - val_accuracy: 0.9842 - val_loss: 0.0472\n",
      "\u001b[1m6796/6796\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 667us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "\n",
    "# Построение базовой нейросети\n",
    "input_dim = X_unsw_train_processed.shape[1]\n",
    "model = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_dim=input_dim),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1, activation='sigmoid')  # Для бинарной классификации\n",
    "])\n",
    "\n",
    "# Компиляция модели\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Обучение\n",
    "history = model.fit(X_unsw_train_processed, y_unsw_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Оценка на тестовых данных\n",
    "y_unsw_pred_probs = model.predict(X_unsw_test_processed)  # Предсказания вероятностей\n",
    "y_unsw_pred_nn = (y_unsw_pred_probs > 0.5).astype(\"int32\")  # Бинарные предсказания\n",
    "\n",
    "# Метрики\n",
    "accuracy_1 = accuracy_score(y_unsw_test, y_unsw_pred_nn)\n",
    "f1_1 = f1_score(y_unsw_test, y_unsw_pred_nn)\n",
    "precision_1 = precision_score(y_unsw_test, y_unsw_pred_nn)\n",
    "recall_1 = recall_score(y_unsw_test, y_unsw_pred_nn)\n",
    "roc_auc_1 = roc_auc_score(y_unsw_test, y_unsw_pred_probs)\n",
    "mcc_1 = matthews_corrcoef(y_unsw_test, y_unsw_pred_nn)\n",
    "cr_1 = classification_report(y_unsw_test, y_unsw_pred_nn, output_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение изначальной модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Сохранение модели\n",
    "model.save(\"baseline_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применение трансферного обучения на BoT-IoT к созданной модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m73371/73371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 2ms/step - accuracy: 0.9836 - loss: 0.2106 - val_accuracy: 0.9999 - val_loss: 0.0044\n",
      "Epoch 2/5\n",
      "\u001b[1m73371/73371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0032 - val_accuracy: 0.9999 - val_loss: 0.0035\n",
      "Epoch 3/5\n",
      "\u001b[1m73371/73371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0025 - val_accuracy: 0.9999 - val_loss: 0.0041\n",
      "Epoch 4/5\n",
      "\u001b[1m73371/73371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0026 - val_accuracy: 0.9999 - val_loss: 0.0037\n",
      "Epoch 5/5\n",
      "\u001b[1m73371/73371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 2ms/step - accuracy: 0.9999 - loss: 0.0023 - val_accuracy: 0.9999 - val_loss: 0.0037\n",
      "\u001b[1m22929/22929\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# --- Трансферное обучение на BoT-IoT ---\n",
    "# Заморозка всех слоев, кроме последних\n",
    "for layer in model.layers[:-1]:\n",
    "    layer.trainable = False  # Замораживаем слои для сохранения обученных весов\n",
    "\n",
    "# Добавление новых слоев, если необходимо адаптировать модель\n",
    "# В данном случае мы используем ту же архитектуру, так что новые слои не добавляются\n",
    "\n",
    "# Компиляция модели для дообучения\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Дообучение на BoT-IoT\n",
    "history_transfer = model.fit(X_bot_train_processed, y_bot_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Оценка модели на тестовом наборе BoT-IoT\n",
    "y_bot_pred_probs = model.predict(X_bot_test_processed)  # Предсказания вероятностей\n",
    "y_bot_pred_nn = (y_bot_pred_probs > 0.5).astype(\"int32\")  # Бинарные предсказания\n",
    "\n",
    "# Метрики для BoT-IoT\n",
    "accuracy_2 = accuracy_score(y_bot_test, y_bot_pred_nn)\n",
    "f1_2 = f1_score(y_bot_test, y_bot_pred_nn)\n",
    "precision_2 = precision_score(y_bot_test, y_bot_pred_nn)\n",
    "recall_2 = recall_score(y_bot_test, y_bot_pred_nn)\n",
    "roc_auc_2 = roc_auc_score(y_bot_test, y_bot_pred_probs)\n",
    "mcc_2 = matthews_corrcoef(y_bot_test, y_bot_pred_nn)\n",
    "cr_2 = classification_report(y_bot_test, y_bot_pred_nn, output_dict=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение дообученной модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Сохранение модели\n",
    "model.save(\"transfer_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрики двух обучений одной модели сохраняются в один файл:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метрики для первого обучения (UNSW-NB15)\n",
    "metrics_unsw = {\n",
    "    \"Metric\": [\"Accuracy\", \"F1 Score\", \"Precision\", \"Recall\", \"ROC-AUC\", \"MCC\"],\n",
    "    \"UNSW-NB15\": [accuracy_1, f1_1, precision_1, recall_1, roc_auc_1, mcc_1]\n",
    "}\n",
    "\n",
    "# Метрики для второго обучения (BoT-IoT)\n",
    "metrics_bot = {\n",
    "    \"Metric\": [\"Accuracy\", \"F1 Score\", \"Precision\", \"Recall\", \"ROC-AUC\", \"MCC\"],\n",
    "    \"BoT-IoT\": [accuracy_2, f1_2, precision_2, recall_2, roc_auc_2, mcc_2]\n",
    "}\n",
    "\n",
    "# Создание датафрейма\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"Metric\": metrics_unsw[\"Metric\"],\n",
    "    \"UNSW-NB15\": metrics_unsw[\"UNSW-NB15\"],\n",
    "    \"BoT-IoT\": metrics_bot[\"BoT-IoT\"]\n",
    "})\n",
    "\n",
    "# Сохранение в CSV\n",
    "metrics_df.to_csv(\"model_metrics_comparison.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохранение отчетов модели в файлы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование в DataFrame\n",
    "report_df = pd.DataFrame(cr_1).transpose()\n",
    "# Сохранение в файл\n",
    "report_df.to_csv(\"classification_report_1.csv\", index=True)\n",
    "\n",
    "report_df = pd.DataFrame(cr_2).transpose()\n",
    "# Сохранение в файл\n",
    "report_df.to_csv(\"classification_report_2.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение классических методов машинного обучения на лимитированных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение модели: Logistic Regression\n",
      "\n",
      "Classification Report для модели Logistic Regression:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       101\n",
      "           1       1.00      1.00      1.00    733604\n",
      "\n",
      "    accuracy                           1.00    733705\n",
      "   macro avg       0.50      0.50      0.50    733705\n",
      "weighted avg       1.00      1.00      1.00    733705\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение модели: Random Forest\n",
      "\n",
      "Classification Report для модели Random Forest:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93       101\n",
      "           1       1.00      1.00      1.00    733604\n",
      "\n",
      "    accuracy                           1.00    733705\n",
      "   macro avg       0.96      0.97      0.96    733705\n",
      "weighted avg       1.00      1.00      1.00    733705\n",
      "\n",
      "Обучение модели: Support Vector Machine\n",
      "\n",
      "Classification Report для модели Support Vector Machine:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.54      0.70       101\n",
      "           1       1.00      1.00      1.00    733604\n",
      "\n",
      "    accuracy                           1.00    733705\n",
      "   macro avg       0.98      0.77      0.85    733705\n",
      "weighted avg       1.00      1.00      1.00    733705\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "# Классические методы машинного обучения\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Support Vector Machine\": SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "# Словарь для сохранения метрик\n",
    "metrics_comparison = []\n",
    "\n",
    "# Обучение и оценка моделей\n",
    "for name, clf in classifiers.items():\n",
    "    print(f\"Обучение модели: {name}\")\n",
    "    \n",
    "    # Обучение модели\n",
    "    clf.fit(X_bot_train_processed, y_bot_train)\n",
    "    \n",
    "    # Предсказания\n",
    "    y_pred = clf.predict(X_bot_test_processed)\n",
    "    y_pred_probs = (\n",
    "        clf.predict_proba(X_bot_test_processed)[:, 1] \n",
    "        if hasattr(clf, \"predict_proba\") else None\n",
    "    )\n",
    "    \n",
    "    # Метрики\n",
    "    accuracy = accuracy_score(y_bot_test, y_pred)\n",
    "    f1 = f1_score(y_bot_test, y_pred)\n",
    "    precision = precision_score(y_bot_test, y_pred)\n",
    "    recall = recall_score(y_bot_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_bot_test, y_pred_probs) if y_pred_probs is not None else None\n",
    "    mcc = matthews_corrcoef(y_bot_test, y_pred)\n",
    "    \n",
    "    # Сохранение метрик\n",
    "    metrics_comparison.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"F1 Score\": f1,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"ROC-AUC\": roc_auc if roc_auc is not None else \"N/A\",\n",
    "        \"MCC\": mcc\n",
    "    })\n",
    "    \n",
    "    # Вывод classification_report\n",
    "    print(f\"\\nClassification Report для модели {name}:\\n\")\n",
    "    print(classification_report(y_bot_test, y_pred))\n",
    "    \n",
    "    # Сохранение classification_report в файл\n",
    "    report = classification_report(y_bot_test, y_pred, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    report_df.to_csv(f\"classification_report_lim_data_{name.replace(' ', '_')}.csv\", index=True)\n",
    "\n",
    "# Создание датафрейма с метриками\n",
    "metrics_df = pd.DataFrame(metrics_comparison)\n",
    "\n",
    "# Сохранение метрик в CSV\n",
    "metrics_csv_path = \"model_metrics_classical_lim_data_comparison.csv\"\n",
    "metrics_df.to_csv(metrics_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соединение всех полученных метрик в один файл:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Объединенные метрики моделей:\n",
      "\n",
      "Metric                   Model  Accuracy  F1 Score  Precision    Recall  \\\n",
      "0                      BoT-IoT  0.999862  0.999931   0.999862  1.000000   \n",
      "1          Logistic Regression  0.999862  0.999931   0.999862  1.000000   \n",
      "2                Random Forest  0.999980  0.999990   0.999990  0.999989   \n",
      "3       Support Vector Machine  0.999935  0.999967   0.999937  0.999997   \n",
      "4                    UNSW-NB15  0.984313  0.603510   0.630556  0.578689   \n",
      "\n",
      "Metric   ROC-AUC       MCC  \n",
      "0       0.862010  0.000000  \n",
      "1       0.836210  0.000000  \n",
      "2       0.999979  0.926109  \n",
      "3       0.941980  0.724852  \n",
      "4       0.956345  0.596095  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Пути к двум CSV-файлам\n",
    "file1 = \"model_metrics_comparison.csv\"  # Файл с метриками UNSW-NB15 и BoT-IoT\n",
    "file2 = \"model_metrics_classical_lim_data_comparison.csv\"  # Файл с метриками классических моделей\n",
    "\n",
    "# Загрузка данных из файлов\n",
    "df1 = pd.read_csv(file1)  # Метрики для UNSW-NB15 и BoT-IoT\n",
    "df2 = pd.read_csv(file2)  # Метрики для классических методов машинного обучения\n",
    "\n",
    "# Преобразование df1: разделяем данные для UNSW-NB15 и BoT-IoT как отдельные модели\n",
    "df1_unsw = df1[[\"Metric\", \"UNSW-NB15\"]].rename(columns={\"UNSW-NB15\": \"Value\"})\n",
    "df1_unsw[\"Model\"] = \"UNSW-NB15\"\n",
    "\n",
    "df1_bot = df1[[\"Metric\", \"BoT-IoT\"]].rename(columns={\"BoT-IoT\": \"Value\"})\n",
    "df1_bot[\"Model\"] = \"BoT-IoT\"\n",
    "\n",
    "# Объединяем метрики из первого файла\n",
    "df1_combined = pd.concat([df1_unsw, df1_bot], ignore_index=True)\n",
    "\n",
    "# Преобразование второго файла (модели уже идут строками)\n",
    "df2_melted = df2.melt(id_vars=[\"Model\"], var_name=\"Metric\", value_name=\"Value\")\n",
    "\n",
    "# Объединяем данные из обоих файлов\n",
    "combined = pd.concat([df1_combined, df2_melted], ignore_index=True)\n",
    "\n",
    "# Преобразуем таблицу в финальный формат: строки — модели, столбцы — метрики\n",
    "final_df = combined.pivot(index=\"Model\", columns=\"Metric\", values=\"Value\").reset_index()\n",
    "\n",
    "# Упорядочиваем столбцы\n",
    "final_df = final_df[[\"Model\", \"Accuracy\", \"F1 Score\", \"Precision\", \"Recall\", \"ROC-AUC\", \"MCC\"]]\n",
    "\n",
    "# Вывод объединенной таблицы в консоль\n",
    "print(\"\\nОбъединенные метрики моделей:\\n\")\n",
    "print(final_df)\n",
    "\n",
    "# Сохранение таблицы в файл\n",
    "output_file = \"final_metrics_table.csv\"\n",
    "final_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Трансферное обучение в данном случае не доказало свою эффективность. Модель случайного леса имеет наилучшие результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfer learning:\n",
      "     Unnamed: 0  precision    recall  f1-score        support\n",
      "0             0   0.000000  0.000000  0.000000     101.000000\n",
      "1             1   0.999862  1.000000  0.999931  733604.000000\n",
      "2      accuracy   0.999862  0.999862  0.999862       0.999862\n",
      "3     macro avg   0.499931  0.500000  0.499966  733705.000000\n",
      "4  weighted avg   0.999725  0.999862  0.999794  733705.000000\n",
      "Logistic Regression:\n",
      "     Unnamed: 0  precision    recall  f1-score        support\n",
      "0             0   0.000000  0.000000  0.000000     101.000000\n",
      "1             1   0.999862  1.000000  0.999931  733604.000000\n",
      "2      accuracy   0.999862  0.999862  0.999862       0.999862\n",
      "3     macro avg   0.499931  0.500000  0.499966  733705.000000\n",
      "4  weighted avg   0.999725  0.999862  0.999794  733705.000000\n",
      "Random_Forest:\n",
      "     Unnamed: 0  precision    recall  f1-score       support\n",
      "0             0   0.921569  0.930693  0.926108     101.00000\n",
      "1             1   0.999990  0.999989  0.999990  733604.00000\n",
      "2      accuracy   0.999980  0.999980  0.999980       0.99998\n",
      "3     macro avg   0.960780  0.965341  0.963049  733705.00000\n",
      "4  weighted avg   0.999980  0.999980  0.999980  733705.00000\n",
      "Support_Vector_Machine:\n",
      "     Unnamed: 0  precision    recall  f1-score        support\n",
      "0             0   0.964912  0.544554  0.696203     101.000000\n",
      "1             1   0.999937  0.999997  0.999967  733604.000000\n",
      "2      accuracy   0.999935  0.999935  0.999935       0.999935\n",
      "3     macro avg   0.982425  0.772276  0.848085  733705.000000\n",
      "4  weighted avg   0.999932  0.999935  0.999925  733705.000000\n"
     ]
    }
   ],
   "source": [
    "cr1 = pd.read_csv('classification_report_2.csv')\n",
    "cr2 = pd.read_csv('classification_report_lim_data_Logistic_Regression.csv')\n",
    "cr3 = pd.read_csv('classification_report_lim_data_Random_Forest.csv')\n",
    "cr4 = pd.read_csv('classification_report_lim_data_Support_Vector_Machine.csv')\n",
    "print(\"Transfer learning:\")\n",
    "print(cr1)\n",
    "print(\"Logistic Regression:\")\n",
    "print(cr2)\n",
    "print(\"Random_Forest:\")\n",
    "print(cr3)\n",
    "print(\"Support_Vector_Machine:\")\n",
    "print(cr4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наилучшая по всем показателям модель случайного леса."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
